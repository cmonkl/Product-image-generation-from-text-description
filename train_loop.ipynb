{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNnGBri6I8QjNEYI0HmRE7a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My\\ Drive/Github/Product-image-generation-from-text-description"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCOcGCB13BTM","executionInfo":{"status":"ok","timestamp":1680637585801,"user_tz":-180,"elapsed":2171,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"88564be7-14d2-4c70-e96a-95a9cdfcb169"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Github/Product-image-generation-from-text-description\n"]}]},{"cell_type":"code","source":["%%writefile train_eval.py\n","from tqdm.auto import tqdm\n","import os\n","import torch\n","\n","\n","def train_step(vae, unet, text_encoder, noise_scheduler, dataloader, criterion, optimizer, device):\n","    unet.train()\n","\n","    epoch_loss = 0.0\n","\n","    for batch_data in dataloader:\n","        text, images = batch_data\n","        images = images.to(device)\n","        with torch.no_grad():\n","            text_embeddings = text_encoder(text[\"input_ids\"].squeeze(1).to(device))[0]\n","        batch_size = images.shape[0]\n","\n","        with torch.no_grad():\n","            latents = vae.encode(images).latent_dist.sample()     \n","            latents = latents * vae.config.scaling_factor\n","        latents = latents.to(device)\n","\n","        # create noise for latents\n","        noise = torch.randn_like(latents).to(device)\n","        # Sample a random timestep for each image\n","        t = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=device).long()\n","        \n","        noisy_images = noise_scheduler.add_noise(latents, noise, t)\n","        noise_pred = unet(noisy_images, t, encoder_hidden_states=text_embeddings).sample\n","\n","        loss = criterion(noise_pred.float(), noise.float())\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        \n","        epoch_loss += loss.item()\n","    \n","    return loss / len(dataloader)\n","\n","def eval_step(vae, unet, text_encoder, noise_scheduler, dataloader, device, height, width):\n","    unet.eval()\n","    vae.eval()\n","    text_encoder.eval()\n","    \n","    metric = 0.0\n","    for batch_data in dataloader:\n","        text, images = batch_data\n","        images = images.to(device)\n","        with torch.no_grad():\n","            text_embeddings = text_encoder(text[\"input_ids\"].squeeze(1).to(device))[0]\n","        batch_size = images.shape[0]\n","\n","        latents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\n","        latents = latents * vae.config.scaling_factor\n","        latents = latents.to(device)\n","        \n","        for t in noise_scheduler.timesteps:\n","            latent_model_input = noise_scheduler.scale_model_input(latents, t)\n","\n","            # predict the noise residual\n","            with torch.no_grad():\n","                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n","\n","            # compute the previous noisy sample x_t -> x_t-1\n","            latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n","        \n","        latents = 1 / vae.config.scaling_factor * latents\n","\n","        with torch.no_grad():\n","            images = vae.decode(latents).sample\n","\n","        # compute metrics\n","        # todo\n","        metric += images.mean()\n","\n","    return metric / len(dataloader)\n","\n","def train(vae, unet, text_encoder, noise_scheduler, num_epochs, train_loader, \n","          val_loader, criterion, optimizer, save_path, logger, device, inf_freq=None):\n","    vae.eval()\n","    text_encoder.eval()\n","    im_height, im_width = val_loader.dataset[0][1].shape[1:3]\n","\n","    best_metric = 0.0\n","    for epoch in range(num_epochs):\n","        train_loss = train_step(vae, unet, text_encoder, noise_scheduler, \n","                                train_loader, criterion, optimizer, device)\n","        \n","        # log train loss to wandb\n","        logger.log({\"train_loss\":train_loss}, step=epoch)\n","\n","        if epoch % inf_freq == 0:\n","            val_metric = eval_step(vae, unet, text_encoder, noise_scheduler,\n","                                   val_loader, device, im_height, im_width)\n","            logger.log({\"val_metric\":val_metric}, step=epoch)\n","\n","            if val_metric > best_metric:\n","                # save best model\n","                torch.save({\n","                    'epoch': epoch,\n","                    'unet_state_dict': unet.state_dict(),\n","                    'vae_state_dict': vae.state_dict(),\n","                    'text_enc_state_dict': text_encoder.state_dict()\n","                    }, os.path.join(save_path, f\"diffusion_model_{round(val_metric, 2)}.pt\"))\n","                \n","                prev_file = os.path.join(save_path, f\"diffusion_model_{round(best_metric)}.pt\")\n","                if os.path.exists(prev_file):\n","                    os.remove(prev_file)\n","                best_metric = val_metric\n","\n","    # load best model weights\n","    best_checkpoint = torch.load(os.path.join(save_path, f\"diffusion_model_{round(best_metric, 2)}.pt\"))\n","    vae.load_state_dict(best_checkpoint[\"vae_state_dict\"])\n","    unet.load_state_dict(best_checkpoint[\"unet_state_dict\"])\n","    text_encoder.load_state_dict(best_checkpoint[\"text_enc_state_dict\"])\n","\n","    model = {'vae': vae, 'unet': unet, \"text_encoder\": text_encoder}\n","    return model\n","\n","def generate_images(text_prompts):\n","    # todo\n","    pass"],"metadata":{"id":"l3LA2W_Qo-Ww","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680637749180,"user_tz":-180,"elapsed":392,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"d244a2cd-8799-44b0-f551-f92c8826fe42"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting train_eval.py\n"]}]},{"cell_type":"code","source":["import os\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, images, texts, tokenizer):\n","        self.images = images\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","\n","    def __getitem__(self, index):\n","        tokenized_text = self.tokenizer(self.texts[index], padding=\"max_length\", \n","                                        max_length=self.tokenizer.model_max_length, \n","                                        truncation=True,\n","                                   return_tensors=\"pt\")\n","\n","        image = self.images[index]\n","        return tokenized_text, image\n","\n","    def __len__(self):\n","        return len(self.images)"],"metadata":{"id":"o89v1yu3arCu","executionInfo":{"status":"ok","timestamp":1680637591492,"user_tz":-180,"elapsed":13,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip install -qq -U diffusers transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kMUUI4nuav0r","executionInfo":{"status":"ok","timestamp":1680636519977,"user_tz":-180,"elapsed":13408,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"bdbb821a-fdcb-4608-ef5b-e6cd37d23c33"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from diffusers import AutoencoderKL \n","from diffusers import UNet2DConditionModel, LMSDiscreteScheduler, DDPMScheduler\n","from transformers import CLIPTextModel, CLIPTokenizer\n","import torch\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n","unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(device)\n","text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n","noise_scheduler = DDPMScheduler(\n","        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n","    \n","def disable_grad(model):\n","    for p in model.parameters(): \n","        p.requires_grad = False \n","        \n","    return model\n","\n","vae.requires_grad_(False)#disable_grad(vae)\n","text_encoder.requires_grad_(False)# = disable_grad(text_encoder)\n","#for p in text_encoder.parameters(): p.requires_grad = False"],"metadata":{"id":"RZi_A-BkcB4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 5\n","train_images = torch.randn((n, 3, 16, 16))\n","val_images = torch.randn((n, 3, 16, 16))\n","train_texts = [f\"text_{i}\" for i in range(n)]\n","val_texts = [f\"text_text{i}\" for i in range(n)]\n","\n","train_dataset = CustomDataset(train_images, train_texts, tokenizer)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2)\n","\n","val_dataset = CustomDataset(val_images, val_texts, tokenizer)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2)"],"metadata":{"id":"siJ5P6C0bDta","executionInfo":{"status":"ok","timestamp":1680637661150,"user_tz":-180,"elapsed":24,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import sys\n","import os\n","\n","path = '/content/drive/MyDrive/Github/Product-image-generation-from-text-description'\n","sys.path.insert(0, path)"],"metadata":{"id":"mquyNWW_X1UD","executionInfo":{"status":"ok","timestamp":1680637661153,"user_tz":-180,"elapsed":21,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["with open(os.path.join(os.path.split(path)[0], \"wandb_token.txt\")) as f:\n","    key = f.read()"],"metadata":{"id":"mnflXWse3VJa","executionInfo":{"status":"ok","timestamp":1680637661154,"user_tz":-180,"elapsed":20,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade -q wandb\n","import wandb\n","wandb.login(key=key)\n","run = wandb.init(project='text-to-image',\n","                    group='finetune', #resume='must',\n","                    job_type='train')"],"metadata":{"id":"CcMpw_HNJxt8","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1680637672246,"user_tz":-180,"elapsed":11109,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"5bea6f18-913d-4eba-8bdd-315d7657012c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mearina\u001b[0m (\u001b[33mdatasatanists\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.14.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/Github/Product-image-generation-from-text-description/wandb/run-20230404_194750-pc73vxcy</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/datasatanists/text-to-image/runs/pc73vxcy' target=\"_blank\">scarlet-firebrand-6</a></strong> to <a href='https://wandb.ai/datasatanists/text-to-image' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/datasatanists/text-to-image' target=\"_blank\">https://wandb.ai/datasatanists/text-to-image</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/datasatanists/text-to-image/runs/pc73vxcy' target=\"_blank\">https://wandb.ai/datasatanists/text-to-image/runs/pc73vxcy</a>"]},"metadata":{}}]},{"cell_type":"code","source":["#from train_eval import train\n","\n","train(vae, unet, text_encoder, noise_scheduler, 3, train_dataloader, val_dataloader, \n","      torch.nn.functional.mse_loss, \n","      torch.optim.Adam(unet.parameters()), \n","      os.path.join(path, 'models'), wandb, device, 1)\n","#wandb.finish()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"k2FRaFqJb-fn","executionInfo":{"status":"error","timestamp":1680637684491,"user_tz":-180,"elapsed":12261,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"31af85ce-9a4b-4f2a-8ec1-87a6447d7360"},"execution_count":9,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-e146b933121b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from train_eval import train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train(vae, unet, text_encoder, noise_scheduler, 3, train_dataloader, val_dataloader, \n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-624431825804>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(vae, unet, text_encoder, noise_scheduler, num_epochs, train_loader, val_loader, criterion, optimizer, save_path, logger, device, inf_freq)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mbest_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         train_loss = train_step(vae, unet, text_encoder, noise_scheduler, \n\u001b[0m\u001b[1;32m     88\u001b[0m                                 train_loader, criterion, optimizer, device)\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-624431825804>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(vae, unet, text_encoder, noise_scheduler, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             self._init_group(\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     90\u001b[0m                     )\n\u001b[1;32m     91\u001b[0m                     \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                     \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg_sq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 14.75 GiB total capacity; 13.20 GiB already allocated; 52.81 MiB free; 13.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]}]}