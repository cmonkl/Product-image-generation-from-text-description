{"cells":[{"cell_type":"code","execution_count":null,"id":"14a877ce","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:50:14.241796Z","iopub.status.busy":"2023-04-21T13:50:14.241435Z","iopub.status.idle":"2023-04-21T13:50:15.076251Z","shell.execute_reply":"2023-04-21T13:50:15.074806Z"},"papermill":{"duration":0.84686,"end_time":"2023-04-21T13:50:15.080117","exception":false,"start_time":"2023-04-21T13:50:14.233257","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"14a877ce","executionInfo":{"status":"ok","timestamp":1682850934896,"user_tz":-180,"elapsed":299,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"8d148bbb-1932-4d70-b7b2-00b9a4587727"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Github/Product-image-generation-from-text-description/code/config.py\n"]}],"source":["%%writefile /content/drive/MyDrive/Github/Product-image-generation-from-text-description/code/config.py\n","from easydict import EasyDict as edict\n","from accelerate import Accelerator\n","from tqdm import tqdm \n","\n","args = edict()\n","\n","args.gradient_accumulation_steps = 2\n","args.mixed_precision = \"fp16\" \n","args.gradient_checkpointing = True\n","\n","accelerator = Accelerator(\n","        gradient_accumulation_steps=args.gradient_accumulation_steps,\n","        mixed_precision=args.mixed_precision,\n","    )\n","args.revision = \"fp16\"\n","args.pretrained_model_name_or_path = 'CompVis/stable-diffusion-v1-4'\n","args.use_8bit_adam = True\n","args.train_batch_size = 8\n","args.max_train_steps = None\n","args.num_train_epochs = 10\n","args.train_text_encoder = False\n","args.set_grads_to_none = False\n","args.seed = None\n","args.scale_lr = False #???????????\n","args.learning_rate = 1e-6\n","args.adam_beta1 = 0.9\n","args.adam_beta2 = 0.999\n","args.adam_weight_decay = 1e-2\n","args.adam_epsilon = 1e-08\n","args.output_dir = '/kaggle/working/'\n","args.height, args.width = test_dataloader.dataset[0][1].shape[1:3]\n","args.num_inference_steps = 50\n","args.enable_xformers_memory_efficient_attention = False\n","args.max_grad_norm = 1.0\n","args.validation_steps = 1\n","args.checkpointing_steps = 7 #args.num_train_epochs // 2 + 1\n","args.lr_scheduler = 'constant'\n","args.lr_warmup_steps = 500\n","args.lr_num_cycles = 1\n","args.lr_power = 1\n","args.revision = \"fp16\"\n","args.resume_from_checkpoint = True\n","args.checkpoint_path = '/kaggle/input/fashion-data/checkpoint-10545/checkpoint-10545'"]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/Github/Product-image-generation-from-text-description/code/train_eval.py\n","\n","from diffusers import (\n","    UNet2DConditionModel, \n","    LMSDiscreteScheduler, \n","    DDPMScheduler,\n","    DPMSolverMultistepScheduler,\n","    DiffusionPipeline,\n","    AutoencoderKL\n",")\n","from transformers import CLIPTextModel, CLIPTokenizer\n","import torch\n","import bitsandbytes as bnb\n","\n","def eval_step(unet, text_encoder, tokenizer, vae, accelerator, dataloader, logger, epoch, args, weight_dtype):\n","    indices = dataloader.dataset.indices\n","    n = 10\n","    labels = [dataloader.dataset.dataset.descriptions.iloc[indices[i]]['description'] for i in range(n)]\n","    true_images = [dataloader.dataset[i][1].float().permute(1, 2, 0) for i in range(n)]\n","    true_images = [(image / 2 + 0.5).clamp(0, 1).numpy() for image in true_images]\n","    image_array = [(true_images[i] * 255).astype(np.uint8) for i in range(len(true_images))]\n","    true_images = [Image.fromarray(image) for image in image_array]\n","\n","    pipeline = DiffusionPipeline.from_pretrained(\n","        args.pretrained_model_name_or_path,\n","        text_encoder=accelerator.unwrap_model(text_encoder),\n","        tokenizer=tokenizer,\n","        unet=accelerator.unwrap_model(unet),\n","        vae=vae,\n","        revision=args.revision,\n","        torch_dtype=weight_dtype,\n","    )\n","    pipeline.safety_checker = lambda images, clip_input: (images, False)\n","    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n","    pipeline = pipeline.to(accelerator.device)\n","    #pipeline.set_progress_bar_config()\n","\n","    # run inference\n","    generator = None if args.seed is None else torch.Generator(device=accelerator.device).manual_seed(args.seed)\n","    images = []\n","    for i in tqdm(range(n)):\n","        with torch.autocast(\"cuda\"):\n","            image = pipeline(labels[i], num_inference_steps=args.num_inference_steps, \n","                             generator=generator, width=args.width, \n","                             height=args.height).images[0]\n","            images.append(image)\n","\n","    logger.log({\"true_images\": [wandb.Image(image, caption=labels[i]) for i, image in enumerate(true_images)],\n","                \"pred_images\": [wandb.Image(image, caption=labels[i]) for i, image in enumerate(images)]},\n","                      step=epoch)\n","    \n","    del pipeline\n","    torch.cuda.empty_cache()\n","\n","def train(args, train_dataloader, val_dataloader):\n","    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", \n","                                    revision=args.revision)#,  torch_dtype=torch.float16)\n","    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\",\n","                                            revision=args.revision)\n","    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, \n","                                                subfolder=\"text_encoder\",\n","                                                revision=args.revision)#,  torch_dtype=torch.float16)\n","\n","    noise_scheduler = DDPMScheduler(\n","            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n","\n","    params_to_optimize = unet.parameters()\n","    optimizer = bnb.optim.AdamW8bit(\n","        params_to_optimize,\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","\n","    if args.seed is not None:\n","    set_seed(args.seed)\n","\n","    vae.requires_grad_(False)\n","    text_encoder.requires_grad_(False)\n","\n","    if args.gradient_checkpointing:\n","        unet.enable_gradient_checkpointing()\n","        if args.train_text_encoder:\n","            text_encoder.gradient_checkpointing_enable()\n","\n","    # Check that all trainable models are in full precision\n","    low_precision_error_string = (\n","        \"Please make sure to always have all model weights in full float32 precision when starting training - even if\"\n","        \" doing mixed precision training. copy of the weights should still be float32.\"\n","    )\n","\n","    if accelerator.unwrap_model(unet).dtype != torch.float32:\n","        raise ValueError(\n","            f\"Unet loaded as datatype {accelerator.unwrap_model(unet).dtype}. {low_precision_error_string}\"\n","        )\n","\n","    if args.train_text_encoder and accelerator.unwrap_model(text_encoder).dtype != torch.float32:\n","        raise ValueError(\n","            f\"Text encoder loaded as datatype {accelerator.unwrap_model(text_encoder).dtype}.\"\n","            f\" {low_precision_error_string}\"\n","        )\n","\n","\n","    if args.scale_lr:\n","        args.learning_rate = (\n","            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n","        )\n","\n","    # Scheduler and math around the number of training steps.\n","    overrode_max_train_steps = False\n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n","    if args.max_train_steps is None:\n","        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n","        overrode_max_train_steps = True\n","\n","    lr_scheduler = get_scheduler(\n","        args.lr_scheduler,\n","        optimizer=optimizer,\n","        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n","        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n","        num_cycles=args.lr_num_cycles,\n","        power=args.lr_power,\n","    )\n","\n","    # Prepare everything with our `accelerator`.\n","    if args.train_text_encoder:\n","        unet, text_encoder, optimizer, train_dataloader, test_dataloader, lr_scheduler = accelerator.prepare(\n","            unet, text_encoder, optimizer, train_dataloader, test_dataloader, lr_scheduler\n","        )\n","    else:\n","        unet, optimizer, train_dataloader, test_dataloader, lr_scheduler = accelerator.prepare(\n","            unet, optimizer, train_dataloader, test_dataloader, lr_scheduler\n","        )\n","\n","    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n","    # as these models are only used for inference, keeping weights in full precision is not required.\n","    weight_dtype = torch.float32\n","    if accelerator.mixed_precision == \"fp16\":\n","        weight_dtype = torch.float16\n","    elif accelerator.mixed_precision == \"bf16\":\n","        weight_dtype = torch.bfloat16\n","\n","    # Move vae and text_encoder to device and cast to weight_dtype\n","    vae.to(accelerator.device, dtype=weight_dtype)\n","    if not args.train_text_encoder:\n","        text_encoder.to(accelerator.device, dtype=weight_dtype)\n","\n","    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n","\n","    if overrode_max_train_steps:\n","        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n","    # Afterwards we recalculate our number of training epochs\n","    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n","\n","    # Train!\n","    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n","\n","    global_step = 0\n","    first_epoch = 0\n","\n","    # Potentially load in the weights and states from a previous save\n","    if args.resume_from_checkpoint:\n","        path = os.path.basename(args.checkpoint_path)\n","\n","        if path is None:\n","            accelerator.print(\n","                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n","            )\n","            args.resume_from_checkpoint = None\n","        else:\n","            accelerator.print(f\"Resuming from checkpoint {path}\")\n","            accelerator.load_state(os.path.join(args.checkpoint_path))\n","            global_step = int(path.split(\"-\")[1])\n","\n","            resume_global_step = global_step * args.gradient_accumulation_steps\n","            first_epoch = global_step // num_update_steps_per_epoch\n","            resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)\n","\n","    # Only show the progress bar once on each machine.\n","    progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\n","    progress_bar.set_description(\"Steps\")\n","\n","    for epoch in range(first_epoch, args.num_train_epochs):\n","        unet.train()\n","        if args.train_text_encoder:\n","            text_encoder.train()\n","        \n","        epoch_loss = 0.0\n","        for step, batch in enumerate(train_dataloader):\n","            if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n","                if step % args.gradient_accumulation_steps == 0:\n","                    progress_bar.update(1)\n","                continue\n","                    \n","            with accelerator.accumulate(unet):\n","                text, images = batch\n","                # Convert images to latent space\n","                latents = vae.encode(images.to(dtype=weight_dtype)).latent_dist.sample()\n","                latents = latents * vae.config.scaling_factor\n","\n","                noise = torch.randn_like(latents)\n","                    \n","                bsz = latents.shape[0]\n","                # Sample a random timestep for each image\n","                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n","                timesteps = timesteps.long()\n","\n","                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","                # Get the text embedding for conditioning\n","                encoder_hidden_states = text_encoder(text[\"input_ids\"].squeeze(1))[0]\n","\n","                # Predict the noise residual\n","                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n","\n","                # Get the target for loss depending on the prediction type\n","                if noise_scheduler.config.prediction_type == \"epsilon\":\n","                    target = noise\n","                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n","                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n","                else:\n","                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n","\n","                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n","                \n","                accelerator.backward(loss)\n","                if accelerator.sync_gradients:\n","                    params_to_clip = (\n","                        itertools.chain(unet.parameters(), text_encoder.parameters())\n","                        if args.train_text_encoder\n","                        else unet.parameters()\n","                    )\n","                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n","                optimizer.step()\n","                lr_scheduler.step()\n","                optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n","\n","            if accelerator.sync_gradients:\n","                progress_bar.update(1)\n","                global_step += 1\n","                \n","                if accelerator.is_main_process:\n","                    if global_step % (args.checkpointing_steps * num_update_steps_per_epoch) == 0:\n","                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n","                        accelerator.save_state(save_path)\n","                        print(f\"Saved state to {save_path}\")\n","\n","                    if global_step % (args.validation_steps * num_update_steps_per_epoch) == 0:\n","                        eval_step(unet, text_encoder, tokenizer, vae, accelerator, test_dataloader, \n","                                args.logger, epoch, args,weight_dtype) \n","                        \n","            logs = {\"train_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n","            progress_bar.set_postfix(**logs)\n","            \n","            epoch_loss += loss.detach().item()\n","            if global_step >= args.max_train_steps:\n","                break\n","\n","        args.logger.log({\"train_loss\": epoch_loss / num_update_steps_per_epoch}, step=epoch)\n","        args.logger.log({\"lr\":lr_scheduler.get_last_lr()[0]}, step=epoch)\n","        print(f\"Epoch: {epoch}, loss: {epoch_loss / num_update_steps_per_epoch}\")\n","\n","    # Create the pipeline using using the trained modules and save it.\n","    accelerator.wait_for_everyone()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95ncD0CerNjS","executionInfo":{"status":"ok","timestamp":1682850838723,"user_tz":-180,"elapsed":220,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"2aca9775-b367-4e4b-8d75-1cee91525010"},"id":"95ncD0CerNjS","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/Github/Product-image-generation-from-text-description/code/train_eval.py\n"]}]},{"cell_type":"code","source":["use_colab = True\n","\n","if use_colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9pTidowuBYD","executionInfo":{"status":"ok","timestamp":1682850813967,"user_tz":-180,"elapsed":19038,"user":{"displayName":"Ирина Мальцева","userId":"12751035672140619453"}},"outputId":"88173112-a8bc-4e81-c837-f554d7916cd6"},"id":"s9pTidowuBYD","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"91a9b318","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:48:43.552439Z","iopub.status.busy":"2023-04-21T13:48:43.551290Z","iopub.status.idle":"2023-04-21T13:49:31.715251Z","shell.execute_reply":"2023-04-21T13:49:31.714217Z"},"papermill":{"duration":48.173051,"end_time":"2023-04-21T13:49:31.718114","exception":false,"start_time":"2023-04-21T13:48:43.545063","status":"completed"},"tags":[],"id":"91a9b318"},"outputs":[],"source":["if not use_colab:\n","    from kaggle_secrets import UserSecretsClient\n","    user_secrets = UserSecretsClient()\n","    key = user_secrets.get_secret(\"wandb_api\")\n","else:\n","    with open('wandb_token.txt') as f:\n","        key = f.read()\n","        \n","!pip install --upgrade wandb\n","import wandb\n","wandb.login(key=key)\n","run = wandb.init(project='text-to-image',\n","                    group='finetune', #resume='must',\n","                    job_type='train')"]},{"cell_type":"code","execution_count":null,"id":"9133287b","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:49:34.517865Z","iopub.status.busy":"2023-04-21T13:49:34.517158Z","iopub.status.idle":"2023-04-21T13:49:37.879507Z","shell.execute_reply":"2023-04-21T13:49:37.878053Z"},"papermill":{"duration":3.37734,"end_time":"2023-04-21T13:49:37.883737","exception":false,"start_time":"2023-04-21T13:49:34.506397","status":"completed"},"tags":[],"id":"9133287b"},"outputs":[],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Subset\n","from transformers import CLIPTokenizer\n","from sklearn.model_selection import train_test_split\n","\n","from code.CustomDataset import CustomTensorDataset\n","from code.config import args"]},{"cell_type":"code","execution_count":null,"id":"4ba48cf9","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:49:37.901260Z","iopub.status.busy":"2023-04-21T13:49:37.900739Z","iopub.status.idle":"2023-04-21T13:49:57.145459Z","shell.execute_reply":"2023-04-21T13:49:57.143752Z"},"papermill":{"duration":19.258698,"end_time":"2023-04-21T13:49:57.149535","exception":false,"start_time":"2023-04-21T13:49:37.890837","status":"completed"},"tags":[],"id":"4ba48cf9"},"outputs":[],"source":["!pip install -qq -U diffusers transformers accelerate"]},{"cell_type":"code","execution_count":null,"id":"baa14f2e","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:50:00.025429Z","iopub.status.busy":"2023-04-21T13:50:00.025122Z","iopub.status.idle":"2023-04-21T13:50:14.222234Z","shell.execute_reply":"2023-04-21T13:50:14.220799Z"},"papermill":{"duration":14.208396,"end_time":"2023-04-21T13:50:14.225974","exception":false,"start_time":"2023-04-21T13:50:00.017578","status":"completed"},"tags":[],"id":"baa14f2e","outputId":"739265e3-de91-4121-b5bb-72a80c576840"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]}],"source":["!pip install -q bitsandbytes"]},{"cell_type":"code","execution_count":null,"id":"8aab4462","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:49:57.164905Z","iopub.status.busy":"2023-04-21T13:49:57.164584Z","iopub.status.idle":"2023-04-21T13:49:57.307099Z","shell.execute_reply":"2023-04-21T13:49:57.305785Z"},"papermill":{"duration":0.153497,"end_time":"2023-04-21T13:49:57.310717","exception":false,"start_time":"2023-04-21T13:49:57.157220","status":"completed"},"tags":[],"id":"8aab4462"},"outputs":[],"source":["import os\n","\n","use_colab = False\n","\n","if use_colab:\n","    path = '/content/drive/MyDrive/Github/Product-image-generation-from-text-description'\n","else:\n","    path = '/kaggle/input/fashion-data'\n","    \n","path_to_descriptions = os.path.join(path, 'descriptions_2.json')\n","descriptions = pd.read_json(path_to_descriptions, orient='records')\n","descriptions['description'] = descriptions['description'].apply(lambda x: x + ' isolated on white background')"]},{"cell_type":"code","execution_count":null,"id":"ca0fa77c","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:49:57.365834Z","iopub.status.busy":"2023-04-21T13:49:57.365540Z","iopub.status.idle":"2023-04-21T13:49:59.250814Z","shell.execute_reply":"2023-04-21T13:49:59.249568Z"},"papermill":{"duration":1.896925,"end_time":"2023-04-21T13:49:59.254910","exception":false,"start_time":"2023-04-21T13:49:57.357985","status":"completed"},"tags":[],"colab":{"referenced_widgets":["3f4993a07ad94364b8fa3481a56e16fb","2b9672238e854c16967044a76d966e40","8b6948a5994945e888ba556356577c4d","779d5592d2b142618c614c3c53612843"]},"id":"ca0fa77c","outputId":"a58c4cee-3516-4e37-8657-f981327d5a62"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f4993a07ad94364b8fa3481a56e16fb","version_major":2,"version_minor":0},"text/plain":["Downloading (…)tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b9672238e854c16967044a76d966e40","version_major":2,"version_minor":0},"text/plain":["Downloading (…)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b6948a5994945e888ba556356577c4d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"779d5592d2b142618c614c3c53612843","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/788 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import CLIPTokenizer\n","\n","tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, \n","                                          subfolder=\"tokenizer\",\n","                                         revision=args.revision)"]},{"cell_type":"code","execution_count":null,"id":"e7774143","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:49:59.271980Z","iopub.status.busy":"2023-04-21T13:49:59.271108Z","iopub.status.idle":"2023-04-21T13:49:59.952163Z","shell.execute_reply":"2023-04-21T13:49:59.950756Z"},"papermill":{"duration":0.692629,"end_time":"2023-04-21T13:49:59.956158","exception":false,"start_time":"2023-04-21T13:49:59.263529","status":"completed"},"tags":[],"id":"e7774143"},"outputs":[],"source":["RESOLUTION = 256\n","\n","data_transformation_images = transforms.Compose([\n","            transforms.Resize((RESOLUTION, RESOLUTION)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, ), (0.5, ))\n","        ])\n","\n","if use_colab:\n","    im_path = 'content/fashion-dataset/images'\n","else:\n","    im_path = '/kaggle/input/fashion-product-images-dataset/fashion-dataset/images'\n","    \n","dataset = CustomTensorDataset(descriptions, tokenizer, im_path, transform_images=data_transformation_images)"]},{"cell_type":"code","execution_count":null,"id":"a69c15fc","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:49:59.972743Z","iopub.status.busy":"2023-04-21T13:49:59.972108Z","iopub.status.idle":"2023-04-21T13:49:59.981307Z","shell.execute_reply":"2023-04-21T13:49:59.980255Z"},"papermill":{"duration":0.020193,"end_time":"2023-04-21T13:49:59.983664","exception":false,"start_time":"2023-04-21T13:49:59.963471","status":"completed"},"tags":[],"id":"a69c15fc"},"outputs":[],"source":["indices = np.arange(len(descriptions))\n","indices_train, indices_test = train_test_split(indices, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"id":"e22c55bd","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:50:00.001163Z","iopub.status.busy":"2023-04-21T13:50:00.000869Z","iopub.status.idle":"2023-04-21T13:50:00.008884Z","shell.execute_reply":"2023-04-21T13:50:00.007142Z"},"papermill":{"duration":0.0207,"end_time":"2023-04-21T13:50:00.011043","exception":false,"start_time":"2023-04-21T13:49:59.990343","status":"completed"},"tags":[],"id":"e22c55bd"},"outputs":[],"source":["train_dataset = Subset(dataset, indices_train)\n","test_dataset = Subset(dataset, indices_test)\n","\n","batch_size = 8\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","args.height, args.width = test_dataloader.dataset[0][1].shape[1:3]\n","args.logger = wandb"]},{"cell_type":"code","source":["train(args)"],"metadata":{"id":"Co98JBRwsv_1"},"id":"Co98JBRwsv_1","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"79d05571","metadata":{"execution":{"iopub.execute_input":"2023-04-21T22:38:40.545035Z","iopub.status.busy":"2023-04-21T22:38:40.544648Z","iopub.status.idle":"2023-04-21T22:39:21.591824Z","shell.execute_reply":"2023-04-21T22:39:21.590233Z"},"papermill":{"duration":42.790782,"end_time":"2023-04-21T22:39:21.596179","exception":false,"start_time":"2023-04-21T22:38:38.805397","status":"completed"},"tags":[],"id":"79d05571"},"outputs":[],"source":["from diffusers import DiffusionPipeline\n","\n","save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n","accelerator.save_state(save_path)\n","print(f\"Saved state to {save_path}\")\n","\n","pipeline = DiffusionPipeline.from_pretrained(\n","    args.pretrained_model_name_or_path,\n","    unet=accelerator.unwrap_model(unet),\n","    text_encoder=accelerator.unwrap_model(text_encoder),\n","    revision=args.revision\n",")\n","pipeline.save_pretrained('/kaggle/working/data/')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":31854.214201,"end_time":"2023-04-21T22:39:28.775301","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-04-21T13:48:34.561100","version":"2.4.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}